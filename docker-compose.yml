version: "3.8"

services:
  # Note: MongoDB is installed system-wide (outside Docker)
  # Backend connects to host MongoDB via host.docker.internal:27017

  # FastAPI Embedding Service
  embedding-service:
    build:
      context: ./ai-embedding-service
      dockerfile: Dockerfile
    container_name: ai-portal-embedding
    restart: unless-stopped
    # Note: env_file is set in override files (docker-compose.dev.yml, docker-compose.staging.yml, docker-compose.prod.yml)
    # Base file only contains default values
    environment:
      # Override or set defaults (env_file takes precedence)
      - ENVIRONMENT=${ENVIRONMENT:-production}
      - API_HOST=${API_HOST:-0.0.0.0}
      - API_PORT=${API_PORT:-8001}
      - EMBEDDING_MODEL=${EMBEDDING_MODEL:-all-MiniLM-L6-v2}
      - DEVICE=${DEVICE:-cpu}
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - CORS_ORIGINS=${CORS_ORIGINS:-*}
    ports:
      - "8001:8001"
    volumes:
      - embedding_cache:/app/.cache
    networks:
      - ai-portal-network
    healthcheck:
      test:
        [
          "CMD",
          "python",
          "-c",
          "import urllib.request; urllib.request.urlopen('http://localhost:8001/api/v1/health')",
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # Node.js Backend
  backend:
    build:
      context: ./api-service
      dockerfile: Dockerfile
    container_name: ai-portal-backend
    restart: unless-stopped
    # Note: env_file is set in override files (docker-compose.dev.yml, docker-compose.staging.yml, docker-compose.prod.yml)
    # Base file only contains default values
    environment:
      # Override or set defaults (env_file takes precedence)
      # All variables loaded from api-service/.env via env_file above
      - NODE_ENV=${NODE_ENV:-production}
      - PORT=${PORT:-5000}
      # MongoDB is system-wide installed, connect via host.docker.internal
      # For Linux (Docker Engine): use 172.17.0.1 instead of host.docker.internal
      - MONGODB_URI=${MONGODB_URI:-mongodb://host.docker.internal:27017/spaceguide-ai-backend}
      - EMBEDDING_API_URL=${EMBEDDING_API_URL:-http://embedding-service:8001}
      - EMBEDDING_PROVIDER=${EMBEDDING_PROVIDER:-fastapi}
      - CHAT_PROVIDER=${CHAT_PROVIDER:-ollama}
      - TEXT_WRITER_PROVIDER=${TEXT_WRITER_PROVIDER:-ollama}
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://ollama:11434}
      - CHROMADB_URL=${CHROMADB_URL:-http://chromadb:8000}
      - OLLAMA_TEXT_WRITER_MODEL=${OLLAMA_TEXT_WRITER_MODEL:-tinyllama}
      - FRONTEND_URL=${FRONTEND_URL:-http://localhost:3000}
      - CORS_ORIGINS=${CORS_ORIGINS:-http://localhost:3000}
      - BASE_URL=${BASE_URL:-http://localhost:5000}
      - RATE_LIMIT_WINDOW_MS=${RATE_LIMIT_WINDOW_MS:-900000}
      - RATE_LIMIT_MAX_REQUESTS=${RATE_LIMIT_MAX_REQUESTS:-100}
      # Secrets (loaded from api-service/.env via env_file above)
      # These are automatically loaded from env_file, no need to set here
      # Only set if you want to override from host environment
      # - ACCESS_TOKEN_SECRET=${ACCESS_TOKEN_SECRET}
      # - REFRESH_TOKEN_SECRET=${REFRESH_TOKEN_SECRET}
      - STRIPE_SECRET_KEY=${STRIPE_SECRET_KEY:-}
      - STRIPE_WEBHOOK_SECRET=${STRIPE_WEBHOOK_SECRET:-}
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - OPENROUTER_API_KEY=${OPENROUTER_API_KEY:-}
    ports:
      - "5000:5000"
    volumes:
      - backend_uploads:/app/uploads
      - backend_images:/app/public/generated-images
      - backend_logs:/app/logs
    depends_on:
      embedding-service:
        condition: service_healthy
      chromadb:
        condition: service_healthy
    # Add extra_hosts to access host machine from container
    # host.docker.internal works on Windows/Mac and Linux with Docker Desktop
    # For Linux Docker Engine, use: "host.docker.internal:host-gateway"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    networks:
      - ai-portal-network
    healthcheck:
      test:
        [
          "CMD",
          "node",
          "-e",
          "require('http').get('http://localhost:5000/health', (r) => {process.exit(r.statusCode === 200 ? 0 : 1)})",
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # Ollama (Optional - for local LLM)
  ollama:
    image: ollama/ollama:latest
    container_name: ai-portal-ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    networks:
      - ai-portal-network
    # GPU support - optional (will use CPU if GPU not available)
    # Uncomment below if you have NVIDIA GPU and want to use it
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3

  # ChromaDB (Vector Database for embeddings)
  chromadb:
    image: chromadb/chroma:latest
    container_name: ai-portal-chromadb
    restart: unless-stopped
    ports:
      - "8000:8000"
    volumes:
      - chromadb_data:/data # Fixed: Changed from /chroma/chroma to /data
    environment:
      - IS_PERSISTENT=TRUE
      - ANONYMIZED_TELEMETRY=FALSE
    networks:
      - ai-portal-network
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "timeout 1 bash -c 'cat < /dev/null > /dev/tcp/localhost/8000' || exit 1",
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

volumes:
  embedding_cache:
    driver: local
  backend_uploads:
    driver: local
  backend_images:
    driver: local
  backend_logs:
    driver: local
  ollama_data:
    driver: local
  chromadb_data:
    driver: local

networks:
  ai-portal-network:
    driver: bridge
